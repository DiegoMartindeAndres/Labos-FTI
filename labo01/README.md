# üñ•Ô∏è Laboratorio 1: Explorando HTTP con Herramientas Pr√°cticas

## üìú Requisitos

Para llevar a cabo este laboratorio, necesitar√°s las siguientes herramientas:

### cURL
Este laboratorio est√° pensado para ejecutar comandos en la consola de Linux si usas. Para **macOS** y **Windows** puedes emplear su versi√≥n de `curl`, que viene instalada por defecto. Ten en cuenta que su sintaxis es ligeramente distinta a la de Linux.  

Tambi√©n puedes usar la versi√≥n web de [curl](https://reqbin.com/curl) si no viene instalado por defecto en tu SO.

### WGet
En Linux, `WGet` viene instalado por defecto.
En **Windows** y **macOS** no viene instalado por defecto.  
- En **Windows**, puedes descargarlo desde el siguiente enlace: [WGet para Windows](https://eternallybored.org/misc/wget/).  
- En **macOS**, puedes instalarlo f√°cilmente con el siguiente comando si tienes Homebrew:  
  ```bash
  brew install wget
  ```

### Postman
Usaremos la herramiento `Postman`, la puedes descargar aqu√≠ para su SO:

- [Postman](https://www.postman.com/downloads/) o una alternativa como [Hoppscotch](https://hoppscotch.io/)
  
  Puedes usar Postman en la web sin instalar nada, por si no lo tienes instalado o no lo puedes instalar en los ordenadores del laboratorio. [Postman online](https://blog.postman.com/announcing-postman-for-the-web-now-in-open-beta/)
  
---

## üìå Objetivos
En este laboratorio aprender√°s a:

- Entender el protocolo HTTP y sus principales caracter√≠sticas.
- Realizar peticiones HTTP con `curl` y Postman.
- Explorar m√©todos HTTP: `GET`, `POST`, `PUT`, `DELETE`.
- Analizar cabeceras HTTP en respuestas y peticiones.
- Interactuar con una API REST p√∫blica.


---

## üåê Conceptos Clave

### üìú Estructura de una Petici√≥n HTTP

```mermaid
graph TD;
    Cliente-->|Solicitud HTTP|Servidor;
    Servidor-->|Respuesta HTTP|Cliente;
```

Una petici√≥n HTTP se compone de:

- **L√≠nea de petici√≥n**: M√©todo, URL y versi√≥n de HTTP.
- **Cabeceras**: Informaci√≥n adicional sobre la solicitud.
- **Cuerpo (opcional)**: Datos enviados en `POST`, `PUT`, etc.

### üì• Respuestas HTTP

```mermaid
graph TD;
    Servidor-->|C√≥digo de estado + Cabeceras + Cuerpo|Cliente;
```

- **C√≥digo de estado**: Indica el resultado (200 OK, 404 Not Found, etc.).
- **Cabeceras**: Informaci√≥n como tipo de contenido o cach√©.
- **Cuerpo**: Datos devueltos en `HTML`, `JSON`, `XML`, etc.

---

## üîç Explorando HTTP con `curl`


### ¬øQu√© es `cURL`?

cURL es una herramienta de l√≠nea de comandos que permite transferir datos desde o hacia un servidor utilizando diversos protocolos, como HTTP, HTTPS y FTP. Es ampliamente utilizada para realizar solicitudes web y probar APIs directamente desde la terminal. Algunos de los modificadores m√°s comunes incluyen:

- `-X` o `--request`: especifica el m√©todo HTTP a utilizar (GET, POST, PUT, DELETE, etc.).
- `-H` o `--header`: permite enviar encabezados HTTP personalizados.
- `-d` o `--data`: env√≠a datos en una solicitud POST.
- `-o` o `--output`: guarda la respuesta en un archivo en lugar de mostrarla en la terminal.
- `-I` o `--head`: obtiene solo los encabezados de la respuesta.
- `-v` o `--verbose`: proporcionando informaci√≥n adicional sobre el proceso de transferencia. Muestra informaci√≥n detallada sobre la conexi√≥n, incluyendo detalles como la resoluci√≥n DNS, la negociaci√≥n SSL/TLS, los encabezados HTTP enviados y recibidos, y otros datos relevantes para la depuraci√≥n y an√°lisis de la comunicaci√≥n con el servidor.
  
Para obtener una lista completa de los modificadores y opciones disponibles, puedes consultar la p√°gina del manual de cURL ejecutando `man curl` en la terminal, o utilizar `curl --help` para ver una lista resumida de las opciones.

A continuaci√≥n, se muestra un ejemplo b√°sico de c√≥mo utilizar cURL para realizar una solicitud GET a una p√°gina web:

```bash
curl https://www.google.com
```

Este comando recuperar√° el contenido de la p√°gina de inicio de "www.google.com" y lo mostrar√° en la terminal. 

A continuaci√≥n vamos a realizar algunas peticiones HTTP con `curl`. Abre una terminal y sigue las instrucciones. 

Para cada respuesta de las que hagamos queremos que seas capaz de responder a las siguientes preguntas:

- ¬øQu√© **direcci√≥n IP** ha resuelto `curl` para el host?  
- ¬øQu√© **protocolo HTTP** se est√° utilizando en la comunicaci√≥n?  
- ¬øCu√°l es el **c√≥digo de estado HTTP** de la respuesta y qu√© indica sobre el resultado de la solicitud?  
- ¬øCu√°l es el **tama√±o** del contenido devuelto en la respuesta?  
- ¬øQu√© tipo de contenido (`Content-Type`) devuelve el servidor?  
- ¬øQu√© informaci√≥n indica el encabezado `Accept` en la petici√≥n?  
- ¬øCu√°ntas solicitudes pueden hacerse antes de alcanzar el l√≠mite de uso?  
- ¬øQu√© **nueva informaci√≥n** ha a√±adido/modificado/eliminado el servidor a los datos que enviamos?  
- ¬øQu√© encabezados indican si la respuesta puede ser almacenada en cach√©?  
- ¬øQu√© informaci√≥n adicional proporciona el servidor sobre s√≠ mismo en los encabezados?  


### 1Ô∏è‚É£ Realizar una petici√≥n `GET`
Ejecuta en la terminal:
```bash
curl -v https://jsonplaceholder.typicode.com/posts/1
```

### 2Ô∏è‚É£ Simular un `POST`

Puedes cambiar a tu gusto los datos del `POST` por lo que quieras.

#### Si usus Linux o MacOS, puedes usar el siguiente comando para simular un `POST`:
```bash
curl -v -X POST https://jsonplaceholder.typicode.com/posts \
     -H "Content-Type: application/json" \
     -d '{"title": "Nuevo post", "body": "Contenido del post", "userId": 1}'
```

#### Si usas Windows:
```bash
curl -v -X POST https://jsonplaceholder.typicode.com/posts ^
     -H "Content-Type: application/json" ^
     -d "{""title"": ""Nuevo post"", ""body"": ""Contenido del post"", ""userId"": 1}"
```

Al ejecutar la petici√≥n `POST`, obtenemos una respuesta detallada del servidor. Analiza la informaci√≥n contenida en la respuesta y responde a las siguientes preguntas:

Responde a estas preguntas analizando la respuesta completa de `curl` y reflexiona sobre su significado.


### 3Ô∏è‚É£ Probar `PUT` y `DELETE`
```bash
curl -X PUT https://jsonplaceholder.typicode.com/posts/1 \
     -H "Content-Type: application/json" \
     -d '{"id": 1, "title": "T√≠tulo actualizado", "body": "Contenido actualizado", "userId": 1}'
```
```bash
curl -X DELETE https://jsonplaceholder.typicode.com/posts/1
```
Analiza las respuestas obtenidas.

---

## üöÄ Usando Postman para Inspeccionar HTTP

1. **Abrir Postman** y crear una nueva petici√≥n.
2. **Seleccionar m√©todo HTTP** (`GET`, `POST`, etc.).
3. **Ingresar URL** (ejemplo: `https://jsonplaceholder.typicode.com/posts` ).
4. **Enviar la solicitud que hemos hecho con `curl`**
5. **Analizar la respuesta** y compararla con la obtenida con `curl`.

---

## üõ†Ô∏è Explorando otras APIs P√∫blica

Utilizaremos la API p√∫blica de [Rick and Morty](https://rickandmortyapi.com/):

1Ô∏è‚É£ Realizar un `GET` para obtener un personaje:
```bash
curl https://rickandmortyapi.com/api/character/1
```
2Ô∏è‚É£ Explorar la respuesta JSON y sus datos.
3Ô∏è‚É£ Probar otros endpoints de la API.

Repite el proceso con Postman.

---

# Ejercicio

Hay un repositorio llamado [Public APIs](https://github.com/public-apis/public-apis) (pincha en √©l para acceder) que recopila APIs p√∫blicas de diferentes categor√≠as. Elige una API que te interese y realiza una petici√≥n `GET` para obtener datos de la misma. Analiza la respuesta y comparte tus hallazgos con tus compa√±eros.

En este caso lo complicado no hacer una petici√≥n con `curl` o Postman, sino leer la documentaci√≥n de la API en cuesti√≥n y saber cual es la direcci√≥n URL a la que debes hacer la petici√≥n y qu√© datos esperar en la respuesta.

A este direcci√≥n URL se le llama **endpoint**.

---
## WGet

`wget` es una herramienta de l√≠nea de comandos en Linux que permite la descarga de archivos desde la web mediante los protocolos HTTP, HTTPS y FTP. Es especialmente √∫til para transferencias no interactivas, como la descarga de archivos o la creaci√≥n de espejos de sitios web completos. ÓàÄciteÓàÇturn0search12ÓàÅ

**Principales modificadores de `wget`:**

- `-r` o `--recursive`: activa la descarga recursiva, permitiendo descargar un sitio web completo siguiendo los enlaces internos.
- `-l` o `--level=NUMBER`: establece la profundidad m√°xima de recursi√≥n; por defecto, es 5.
- `-k` o `--convert-links`: convierte los enlaces para que funcionen localmente, √∫til para navegar por el sitio descargado sin conexi√≥n.
- `-p` o `--page-requisites`: descarga todos los elementos necesarios para que una p√°gina se muestre correctamente, como im√°genes y hojas de estilo.
- `-P` o `--directory-prefix=PREFIX`: especifica el directorio donde se guardar√°n los archivos descargados.

**Ejemplo b√°sico: Descargar una √∫nica p√°gina web**

Para descargar una sola p√°gina web y los recursos necesarios para su correcta visualizaci√≥n:

```bash
wget -p https://www.ejemplo.com/pagina.html
```

Este comando descargar√° `pagina.html` junto con las im√°genes, hojas de estilo y otros recursos asociados, almacen√°ndolos en el directorio actual.

**Ejemplo avanzado: Descargar un sitio web de forma recursiva**

Para descargar un sitio web completo de manera recursiva, conservando la estructura de enlaces para su navegaci√≥n offline:

```bash
wget -r -k -p -P ./mi_sitio https://www.ejemplo.com
```

Este comando realizar√° lo siguiente:

- `-r`: Descarga recursiva de todo el sitio.
- `-k`: Convierte los enlaces para que apunten a los archivos locales, facilitando la navegaci√≥n sin conexi√≥n.
- `-p`: Descarga todos los elementos necesarios para la correcta visualizaci√≥n de cada p√°gina.
- `-P ./mi_sitio`: Guarda todos los archivos en el directorio `./mi_sitio`.

De esta manera, obtendr√°s una copia local completa del sitio web, almacenada en el directorio especificado, lista para ser navegada sin conexi√≥n.

Para m√°s informaci√≥n y opciones avanzadas, puedes consultar la documentaci√≥n oficial de `wget`. 

## Ejemplo de utilizaci√≥n de `wget`

Vamos a usar la web del curso para hacer pruebas con `wget`. 

(https://ajgallego.github.io/programacion-web/)[https://ajgallego.github.io/programacion-web/]

Para descargar una √∫nica p√°gina web y los recursos necesarios para su correcta visualizaci√≥n, puedes utilizar el siguiente comando:

```bash
wget -p -k -P ./mi_pagina https://ajgallego.github.io/programacion-web/
```

Este comando realiza las siguientes acciones:

- `-p`: Descarga todos los elementos necesarios para que la p√°gina se muestre correctamente, como im√°genes y hojas de estilo.
- `-k`: Convierte los enlaces para que funcionen localmente, facilitando la navegaci√≥n sin conexi√≥n.
- `-P ./mi_pagina`: Guarda los archivos descargados en el directorio `./mi_pagina`.

Para descargar el sitio web de forma recursiva, incluyendo todas las p√°ginas enlazadas y manteniendo la estructura de directorios, puedes utilizar el siguiente comando:

```bash
wget -r -l inf -k -p -P ./mi_sitio https://ajgallego.github.io/programacion-web/
```

Este comando realiza las siguientes acciones:

- `-r`: Activa la descarga recursiva, permitiendo descargar el sitio completo siguiendo los enlaces internos.
- `-l inf`: Establece la profundidad de recursi√≥n a infinita, asegurando que se descarguen todas las p√°ginas enlazadas.
- `-k`: Convierte los enlaces para que funcionen localmente, facilitando la navegaci√≥n sin conexi√≥n.
- `-p`: Descarga todos los elementos necesarios para que cada p√°gina se muestre correctamente, como im√°genes y hojas de estilo.
- `-P ./mi_sitio`: Guarda todos los archivos descargados en el directorio `./mi_sitio`.

De esta manera, obtendr√°s una copia local completa del sitio web, almacenada en el directorio especificado, lista para ser navegada sin conexi√≥n. 


### Comprobar el funcionamiento de WGET

Accede al directorio `./mi_sitio` o el directorio que hayas elegido para la descarga de la web y comprueba que los archivos se han descargado correctamente. Puedes abrir el archivo HTML principal en un navegador para ver c√≥mo se ve la p√°gina web sin conexi√≥n.


